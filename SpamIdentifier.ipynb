{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<a href=\"https://colab.research.google.com/drive/1MOfgIh1XEKNPeov-RDPYmhe0xuPSyqL0?usp=drive_link\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ],
      "metadata": {
        "id": "qaXKuWpsn8v4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spam Identifier\n",
        "\n",
        "This classifier is designed to identify spam messages using artificial intelligence. It is implemented in Python, primarily utilizing the PyTorch and scikit-learn libraries for machine learning and natural language processing tasks."
      ],
      "metadata": {
        "id": "O8ssaikflPdC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell imports the necessary libraries for the spam classifier. `torch` and `torch.nn` are used for creating the neural network model. `Dataset` and `DataLoader` from `torch.utils.data` are used for creating a custom dataset and loading data in batches. `CountVectorizer` from `sklearn.feature_extraction.text` is used for converting text data into numerical vectors. The `csv` library is used for reading the spam dataset. `time` is used for training time calculation, while `re` and `sys` are here only to format the training log better."
      ],
      "metadata": {
        "id": "ef_2dhIJlZbw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "oTi5FHN8WFNU"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "import csv\n",
        "import time\n",
        "import re, sys\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `SpamClassifier` class which is a subclass of `nn.Module`. This class represents the neural network model for the spam classifier. The model consists of two linear layers and uses sigmoid activation function."
      ],
      "metadata": {
        "id": "N6BDMOdellVV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamClassifier(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(SpamClassifier, self).__init__()\n",
        "\n",
        "        self.linear1 = nn.Linear(input_size, 16)\n",
        "        self.linear2 = nn.Linear(16, 1)\n",
        "        self.activation = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear1(x)\n",
        "        x = self.activation(x)\n",
        "        x = self.linear2(x)\n",
        "        x = self.activation(x)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "Bv_OaPgxWYgK"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `SpamDataset` class which is a subclass of `Dataset`. This class represents the spam dataset. It reads the data from a CSV file and converts the text messages into numerical vectors using `CountVectorizer`. The labels are also converted into integers."
      ],
      "metadata": {
        "id": "Uldz2xizltUT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SpamDataset(Dataset):\n",
        "    def __init__(self, csv_file):\n",
        "        self.data = []\n",
        "        self.vectorizer = CountVectorizer()\n",
        "\n",
        "        messages = []\n",
        "        labels = []\n",
        "        with open(csv_file, \"r\") as f:\n",
        "            csv_reader = csv.reader(f)\n",
        "            for row in csv_reader:\n",
        "                if len(row) == 2:\n",
        "                    label, message = row\n",
        "                    messages.append(message)\n",
        "                    labels.append(int(label == 'spam'))  # Convert label to integer\n",
        "\n",
        "        # Convert messages to vectors\n",
        "        message_vectors = self.vectorizer.fit_transform(messages).toarray()\n",
        "\n",
        "        for vector, label in zip(message_vectors, labels):\n",
        "            self.data.append((vector, label))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        message_vector, label = self.data[idx]\n",
        "        return torch.tensor(message_vector, dtype=torch.float32), torch.tensor(label, dtype=torch.float32)"
      ],
      "metadata": {
        "id": "fk3_0rqrWZkL"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell defines the `train` function which trains the model on the training data. It uses a specified optimizer and loss function. The training is done for a specified number of epochs."
      ],
      "metadata": {
        "id": "K6w9dRSul0ph"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Reprinter:\n",
        "\n",
        "    def __init__(self):\n",
        "        self.text = ''\n",
        "\n",
        "    def clear_line(self):\n",
        "        \"\"\"Clears the line before printing the new text.\"\"\"\n",
        "        sys.stdout.write('\\033[F')  # Move cursor up one line\n",
        "        sys.stdout.write('\\r' + ' ' * len(self.text))\n",
        "\n",
        "    def __call__(self, text):\n",
        "        \"\"\"Prints `text` and clears the previous line.\"\"\"\n",
        "        self.clear_line()\n",
        "        print(text, end='', flush=True)\n",
        "        self.text = text\n",
        "        sys.stdout.flush()\n",
        "\n",
        "\n",
        "\n",
        "def train(model, train_data, train_loader, optimizer, loss_fn, epochs):\n",
        "    reprint = Reprinter()\n",
        "    start_time = time.time()  # Record the start time of training\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for batch_idx, (data, target) in enumerate(train_loader):\n",
        "            data = data.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            output = model(data).squeeze()  # Remove the extra dimension from output\n",
        "            loss = loss_fn(output, target)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            if batch_idx % 10 == 0:\n",
        "                reprint(\"Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\".format(\n",
        "                    epoch, batch_idx * len(data), len(train_data), 100.0 * batch_idx / len(train_loader), loss.item()\n",
        "                ))\n",
        "\n",
        "    end_time = time.time()  # Record the end time of training\n",
        "    elapsed_time = end_time - start_time  # Calculate the elapsed time\n",
        "    print(f\"\\nTraining took approximately {elapsed_time:.2f} seconds\")\n",
        "\n",
        "\n",
        "# Define the predict function\n",
        "def predict(model, message):\n",
        "    # Convert the input message to a tensor\n",
        "    message_vector = torch.tensor(train_dataset.vectorizer.transform([message]).toarray(), dtype=torch.float32).to(device)\n",
        "\n",
        "    # Move the model to the appropriate device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Make the prediction\n",
        "    output = model(message_vector)\n",
        "    confidence = output.item() * 100.0\n",
        "\n",
        "    return confidence\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "if torch.cuda.is_available():\n",
        "  deviceName = \"GPU\"\n",
        "else:\n",
        "  deviceName = \"CPU\""
      ],
      "metadata": {
        "id": "AowYegESWZu6"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This cell above defines the `predict` function which makes a prediction on a given message using the trained model. The cell below sets up and trains the model using the training data from the selected dataset."
      ],
      "metadata": {
        "id": "z7ia94jtmBbi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Training Settings\n",
        "choice = \"Small (469 KB)\" # @param [\"Small (469 KB)\", \"Medium (10 MB)\"]\n",
        "model_name = \"spam_classifier_medium\" # @param {type:\"string\"}\n",
        "train_now = True # @param {type:\"boolean\"}\n",
        "if train_now:\n",
        "  print(\"Using\", deviceName)\n",
        "  print(\"____________________________________________________________\")\n",
        "\n",
        "csv.field_size_limit(sys.maxsize)\n",
        "if choice==\"Small (469 KB)\":\n",
        "  train_dataset = SpamDataset(\"/content/sample_data/spam.csv\")\n",
        "elif choice==\"Medium (10 MB)\":\n",
        "  train_dataset = SpamDataset(\"/content/sample_data/spam_20.csv\")\n",
        "elif choice==\"Large (36 MB)\":\n",
        "  train_dataset = SpamDataset(\"spam_large.csv\")\n",
        "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
        "\n",
        "input_size = len(train_dataset.vectorizer.get_feature_names_out())\n",
        "print(f\"INPUT_SIZE FOR {model_name}: {input_size}\")\n",
        "model = SpamClassifier(input_size)\n",
        "model.to(device)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "loss_fn = nn.BCELoss()\n",
        "\n",
        "if train_now:\n",
        "  train(model, train_dataset, train_loader, optimizer, loss_fn, epochs=100)\n",
        "\n",
        "  # Save the model\n",
        "  torch.save(model.state_dict(), f\"{model_name}.pt\")"
      ],
      "metadata": {
        "id": "bwgrQNeIWZ5z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83e4fb62-e759-4cdf-def9-418e342a2956"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using GPU\n",
            "____________________________________________________________\n",
            "INPUT_SIZE FOR spam_classifier_medium: 8709\n",
            "                                                Train Epoch: 99 [5440/5573 (97%)]\tLoss: 0.000002\n",
            "Training took approximately 58.79 seconds\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Try The Model Out\n",
        "\n",
        "Feel free to try the model out in the cell below! Simply enter your message in the text field on the right."
      ],
      "metadata": {
        "id": "LnODJ__PmFx0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uSFLL1Am0rTR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Inference { form-width: \"50%\" }\n",
        "text_input = \"Can you send me the report by tomorrow morning?\" # @param {type:\"string\"}\n",
        "accuracy = 4 # @param {type:\"slider\", min:1, max:13, step:1}\n",
        "model_select = \"spam_classifier_small\" # @param [\"spam_classifier_medium\", \"spam_classifier_small\"]\n",
        "\n",
        "# Load the model\n",
        "if model_select == \"spam_classifier_small\":\n",
        "  train_dataset = SpamDataset(\"/content/sample_data/spam.csv\")\n",
        "elif model_select == \"spam_classifier_medium\":\n",
        "  train_dataset = SpamDataset(\"/content/sample_data/spam_20.csv\")\n",
        "model = SpamClassifier(input_size)\n",
        "if model_select == \"spam_classifier_small\":\n",
        "  model.load_state_dict(torch.load(\"/content/sample_data/spam_classifier_small.pt\", map_location=torch.device('cpu'), weights_only=True))\n",
        "elif model_select == \"spam_classifier_medium\":\n",
        "  model.load_state_dict(torch.load(\"/content/sample_data/spam_classifier_medium.pt\", map_location=torch.device('cpu'), weights_only=True))\n",
        "\n",
        "\n",
        "# Make a prediction\n",
        "message = text_input\n",
        "confidence = predict(model, message)\n",
        "confidence = round(confidence, accuracy)\n",
        "print(f\"Confidence rate: {confidence}%\")"
      ],
      "metadata": {
        "id": "vbLPGmvPWaEp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1477996a-69f1-488e-9de3-590f000f76b7"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Confidence rate: 0.0%\n"
          ]
        }
      ]
    }
  ]
}